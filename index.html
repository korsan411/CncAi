<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>CncAi — Depth (AI) Demo</title>
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <style>
    body { font-family: Inter, Arial, sans-serif; margin: 12px; background:#f6f7fb; color:#111; }
    h1 { font-size:18px; margin:0 0 8px; }
    .row { display:flex; gap:12px; flex-wrap:wrap; align-items:flex-start; }
    .col { background:#fff; padding:10px; border-radius:8px; box-shadow:0 1px 4px rgba(0,0,0,0.06); }
    canvas { display:block; background:#222; }
    #viewer3d { width:480px; height:360px; background:#e9eef6; }
    label { font-size:13px; display:block; margin-bottom:6px; }
    input[type=file] { margin-bottom:8px; }
    .controls { margin-bottom:8px; }
    .muted { font-size:12px; color:#666; margin-top:6px; }
    button { padding:8px 12px; margin-right:8px; cursor:pointer; }
    input[type=text] { width:420px; padding:6px; }
    #log { font-family:monospace; font-size:12px; color:#444; white-space:pre-wrap; margin-top:8px; max-width:980px; }
  </style>
</head>
<body>
  <h1>CncAi — Depth-from-Image (AI + Fallback)</h1>

  <div class="col controls">
    <label>1) ارفع صورة صغيرة (يفضل ≤512×512) — جرب صورة رمادية أو شعار أولاً:</label>
    <input id="file" type="file" accept="image/*" />

    <label>2) اختيار وضع التحليل:</label>
    <select id="mode">
      <option value="fallback">Fallback (brightness → depth) — سريع ومضمون</option>
      <option value="ai">AI (ONNX) — يحتاج موديل ONNX عام (اختياري)</option>
    </select>

    <div id="modelRow" style="margin-top:8px; display:none;">
      <label>Model URL (ONNX file public URL):</label>
      <input id="modelUrl" type="text" placeholder="https://.../midas-small.onnx" />
      <div class="muted">ضع رابط موديل ONNX (إن وُجد). إن لم يوجد، استخدم Fallback.</div>
    </div>

    <div style="margin-top:8px;">
      <button id="runBtn">Run (تحليل → 3D)</button>
      <button id="resetBtn">Reset</button>
    </div>

    <div id="log"></div>
  </div>

  <div class="row" style="margin-top:12px;">
    <div class="col">
      <label>Original (2D)</label>
      <canvas id="origCanvas" width="400" height="300"></canvas>
    </div>

    <div class="col">
      <label>Depth Map (grayscale)</label>
      <canvas id="depthCanvas" width="400" height="300"></canvas>
    </div>

    <div class="col">
      <label>3D Preview (relief)</label>
      <div id="viewer3d"></div>
    </div>
  </div>

  <!-- ONNX runtime (loaded only if AI mode selected) and Three.js -->
  <script src="https://cdn.jsdelivr.net/npm/three@0.152.2/build/three.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/three@0.152.2/examples/js/controls/OrbitControls.js"></script>

  <script>
  // --- helper UI/log ---
  const logEl = document.getElementById('log');
  function log(...args){ console.log(...args); logEl.textContent += args.join(' ') + '\\n'; }

  const fileInput = document.getElementById('file');
  const runBtn = document.getElementById('runBtn');
  const resetBtn = document.getElementById('resetBtn');
  const modeSel = document.getElementById('mode');
  const modelRow = document.getElementById('modelRow');
  const modelUrlInput = document.getElementById('modelUrl');

  modeSel.addEventListener('change', ()=> {
    modelRow.style.display = modeSel.value === 'ai' ? 'block' : 'none';
  });

  // canvases
  const origCanvas = document.getElementById('origCanvas');
  const depthCanvas = document.getElementById('depthCanvas');
  const origCtx = origCanvas.getContext('2d');
  const depthCtx = depthCanvas.getContext('2d');

  // 3D setup (Three.js)
  const container3d = document.getElementById('viewer3d');
  let scene, camera, renderer, controls, surfaceMesh;
  function init3D(){
    // clear old
    container3d.innerHTML = '';
    scene = new THREE.Scene();
    scene.background = new THREE.Color(0xf0f4fb);

    const W = container3d.clientWidth || 480;
    const H = container3d.clientHeight || 360;
    camera = new THREE.PerspectiveCamera(45, W/H, 0.1, 2000);
    camera.position.set(0, -120, 160);

    renderer = new THREE.WebGLRenderer({antialias:true});
    renderer.setSize(W,H);
    container3d.appendChild(renderer.domElement);

    controls = new THREE.OrbitControls(camera, renderer.domElement);
    controls.target.set(0,0,0);

    scene.add(new THREE.AmbientLight(0xffffff, 0.6));
    const dl = new THREE.DirectionalLight(0xffffff, 0.9);
    dl.position.set(40, 60, 100);
    scene.add(dl);

    animate();
  }
  function animate(){
    requestAnimationFrame(animate);
    if (controls) controls.update();
    if (renderer && scene && camera) renderer.render(scene, camera);
  }
  init3D();

  // file image data holder
  let loadedImage = null;
  fileInput.addEventListener('change', (ev)=>{
    const f = ev.target.files[0];
    if(!f) return;
    const url = URL.createObjectURL(f);
    const img = new Image();
    img.onload = ()=> {
      // draw to orig canvas (fit)
      const W = origCanvas.width, H = origCanvas.height;
      // maintain aspect fit
      const scale = Math.min(W/img.width, H/img.height);
      const dw = Math.round(img.width*scale), dh = Math.round(img.height*scale);
      const dx = Math.round((W-dw)/2), dy = Math.round((H-dh)/2);
      origCtx.clearRect(0,0,W,H);
      origCtx.drawImage(img, 0,0, img.width, img.height, dx,dy, dw,dh);
      // save mapped small version (we'll process with the drawn canvas to keep consistent dims)
      loadedImage = { image: img, drawRect: {dx,dy,dw,dh}, canvasW: W, canvasH: H };
      log('Image loaded:', img.width+'x'+img.height, 'drawn', dw+'x'+dh);
      depthCtx.clearRect(0,0,depthCanvas.width, depthCanvas.height);
      // remove old 3D mesh
      if(surfaceMesh){ scene.remove(surfaceMesh); surfaceMesh.geometry.dispose(); surfaceMesh.material.dispose(); surfaceMesh = null; }
    };
    img.onerror = ()=> { log('Error loading image'); };
    img.src = url;
  });

  // Helper: compute depth from imageData using simple brightness fallback
  function computeDepthFallback(imageData, w, h){
    const depth = new Float32Array(w*h);
    const ddata = imageData.data;
    for(let y=0;y<h;y++){
      for(let x=0;x<w;x++){
        const i = (y*w + x)*4;
        // luminance
        const r = ddata[i], g = ddata[i+1], b = ddata[i+2];
        const lum = 0.2126*r + 0.7152*g + 0.0722*b;
        depth[y*w + x] = lum / 255; // 0..1
      }
    }
    return depth;
  }

  // Render depth to depthCanvas (grayscale)
  function renderDepthCanvas(depth, w, h){
    const img = depthCtx.createImageData(w,h);
    for(let i=0;i<w*h;i++){
      const v = Math.max(0, Math.min(255, Math.round(depth[i]*255)));
      img.data[i*4+0] = v;
      img.data[i*4+1] = v;
      img.data[i*4+2] = v;
      img.data[i*4+3] = 255;
    }
    // scale to depthCanvas size (orig and depth canvas have same dims)
    // draw into a temporary canvas then stretch (we assume same size)
    depthCtx.putImageData(img, 0, 0);
  }

  // Build 3D mesh from depth array (w x h)
  function buildSurfaceFromDepth(depth, w, h, scale = 60){
    // remove old
    if(surfaceMesh){ scene.remove(surfaceMesh); surfaceMesh.geometry.dispose(); surfaceMesh.material.dispose(); surfaceMesh = null; }

    // Create plane geometry with subdivisions matching depth resolution
    const geo = new THREE.PlaneGeometry(w, h, w-1, h-1);
    const pos = geo.attributes.position;
    // Note: PlaneGeometry created in X horizontal, Y vertical; we'll rotate for nicer view
    for(let i=0;i<pos.count;i++){
      const z = depth[i] * scale; // scale height
      pos.setZ(i, z);
    }
    geo.computeVertexNormals();

    const mat = new THREE.MeshStandardMaterial({ color:0xdcdcdc, metalness:0.0, roughness:0.8, side:THREE.DoubleSide });
    surfaceMesh = new THREE.Mesh(geo, mat);

    // center and rotate to match camera orientation
    surfaceMesh.rotation.x = -Math.PI/2; // make Z up
    // move so center at origin
    surfaceMesh.position.set(0, 0, 0);
    scene.add(surfaceMesh);

    // fit camera target and position
    controls.target.set(0,0,0);
    // place camera a bit back and up relative to plane size
    camera.position.set(0, -Math.max(w,h)*0.9, Math.max(w,h)*0.6);
    controls.update();
  }

  // AI via ONNX Runtime Web — optional and advanced
  // We'll lazy-load ort.js only if user selects AI mode and provides a model URL.
  let ort = null;
  let session = null;

  async function loadOrt(){
    if(window.ort) { ort = window.ort; return; }
    log('Loading ONNX runtime (ort-web) from CDN...');
    // load script dynamically
    await new Promise((resolve, reject)=>{
      const s = document.createElement('script');
      s.src = 'https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js';
      s.onload = ()=> { ort = window.ort; log('ort loaded'); resolve(); };
      s.onerror = (e)=> { reject(new Error('Failed to load ort')); };
      document.head.appendChild(s);
    });
  }

  // NOTE: running MiDaS in ONNX inside browser requires a model that accepts expected input dims.
  // Implementing a universal MiDaS pre/postprocessing pipeline is sizeable — here we provide a simple
  // hook: if user supplies an ONNX model URL compatible with inputs below, we'll try to run it.
  // If it fails, we fallback gracefully to brightness method.
  async function runAIModelOnImage(modelUrl, srcImage){
    try{
      await loadOrt();
    }catch(err){
      log('ort load failed:', err.message);
      throw err;
    }
    if(!ort){
      throw new Error('ort not available');
    }

    // Create session
    log('Creating ONNX session — this may take a while...');
    try{
      session = await ort.InferenceSession.create(modelUrl);
    }catch(e){
      log('Failed to create ONNX session:', e.message);
      throw e;
    }
    log('Session created. Preparing input...');

    // Preprocess: here we make a simple preproc — many MiDaS models expect specific sizes (e.g. 256/384).
    // We'll resize the image to 256x256 for the model input (user model must match this).
    const T = 256;
    const tmp = document.createElement('canvas');
    tmp.width = T; tmp.height = T;
    const tctx = tmp.getContext('2d');
    // draw image to square center-cropped or fitted
    tctx.fillStyle = '#000'; tctx.fillRect(0,0,T,T);
    // fill with image centered
    const scale = Math.min(T/srcImage.width, T/srcImage.height);
    const dw = Math.round(srcImage.width*scale), dh = Math.round(srcImage.height*scale);
    const dx = Math.round((T-dw)/2), dy = Math.round((T-dh)/2);
    tctx.drawImage(srcImage, 0,0, srcImage.width, srcImage.height, dx, dy, dw, dh);

    const imgData = tctx.getImageData(0,0,T,T);
    // prepare float32 tensor: [1,3,H,W] normalized (0..1 or model-specific). We use 0..1
    const float32 = new Float32Array(1*3*T*T);
    let ptr = 0;
    // convert to CHW
    for(let c=0;c<3;c++){
      for(let y=0;y<T;y++){
        for(let x=0;x<T;x++){
          const i = (y*T + x)*4;
          const v = imgData.data[i + (0) + (c===0?0: (c===1?1:2))]; // simplified channel mapping
          float32[ptr++] = imgData.data[i + (c===0?0:(c===1?1:2))] / 255.0;
        }
      }
    }

    // WARNING: The model input name and shape vary. Try detecting input name:
    const inputNames = session.inputNames || Object.keys(session.inputMaps || {});
    // Fallback: pick first input
    const inputName = session.inputNames && session.inputNames.length>0 ? session.inputNames[0] : session.inputNames;
    const tensor = new ort.Tensor('float32', float32, [1,3,T,T]);
    const feeds = {};
    feeds[inputName] = tensor;

    log('Running inference (this may be slow on CPU)...');
    let results = null;
    try{
      results = await session.run(feeds);
    }catch(e){
      log('ONNX inference failed:', e.message);
      throw e;
    }

    // get first output tensor
    const outName = Object.keys(results)[0];
    const outTensor = results[outName];
    // outTensor.data shape depends on model; try to collapse to HxW by taking first channel
    const outData = outTensor.data;
    // assume outTensor dims [1,1,T,T] or [1,T,T]
    // convert to Float32Array depth map normalized 0..1
    let depthArray = null;
    if(outTensor.dims.length === 4){
      const H = outTensor.dims[2], W = outTensor.dims[3];
      depthArray = new Float32Array(W*H);
      // outData layout: N,C,H,W -> take channel 0
      for(let y=0;y<H;y++){
        for(let x=0;x<W;x++){
          depthArray[y*W + x] = outData[0* (outTensor.dims[1]*H*W) + 0*(H*W) + y*W + x];
        }
      }
      return {depth: depthArray, w: outTensor.dims[3], h: outTensor.dims[2]};
    } else if(outTensor.dims.length === 3){
      const H = outTensor.dims[1], W = outTensor.dims[2];
      depthArray = new Float32Array(W*H);
      for(let y=0;y<H;y++){
        for(let x=0;x<W;x++){
          depthArray[y*W + x] = outData[y*W + x];
        }
      }
      return {depth: depthArray, w: W, h: H};
    } else {
      throw new Error('Unexpected output tensor dims: ' + outTensor.dims);
    }
  }

  // Main run handler
  runBtn.addEventListener('click', async ()=>{
    logEl.textContent = ''; // clear log
    if(!loadedImage){ log('No image loaded'); return; }
    const mode = modeSel.value;
    log('Running mode:', mode);
    // we'll use the origCanvas drawn image as source — get its actual drawn region (we used full canvas)
    const W = origCanvas.width, H = origCanvas.height;
    const imageData = origCtx.getImageData(0,0,W,H);

    if(mode === 'fallback'){
      // simple path
      const depth = computeDepthFallback(imageData, W, H);
      renderDepthCanvas(depth, W, H);
      buildSurfaceFromDepth(depth, W, H, 40); // scale 40 (tweakable)
      log('Fallback depth map used.');
      return;
    }

    // AI path
    const modelUrl = modelUrlInput.value.trim();
    if(!modelUrl){
      log('No model URL provided — switching to fallback.');
      const depth = computeDepthFallback(imageData, W, H);
      renderDepthCanvas(depth, W, H);
      buildSurfaceFromDepth(depth, W, H, 40);
      return;
    }

    try{
      log('Attempting AI depth estimation using ONNX model at:', modelUrl);
      const aiResult = await runAIModelOnImage(modelUrl, loadedImage.image);
      // aiResult.depth values may be unnormalized; normalize 0..1
      const {depth, w, h} = aiResult;
      // normalize min..max to 0..1
      let min=Infinity, max=-Infinity;
      for(let i=0;i<depth.length;i++){ if(depth[i]<min) min=depth[i]; if(depth[i]>max) max=depth[i]; }
      const norm = new Float32Array(depth.length);
      const rng = max - min || 1;
      for(let i=0;i<depth.length;i++) norm[i] = (depth[i]-min)/rng;
      // render and build (we will stretch AI map to our canvas size)
      // create temp canvas to upscale depth to our depthCanvas size
      const tmp = document.createElement('canvas'); tmp.width = w; tmp.height = h;
      const tctx = tmp.getContext('2d');
      const imgData = tctx.createImageData(w,h);
      for(let i=0;i<w*h;i++){
        const v = Math.round(norm[i]*255);
        imgData.data[i*4+0]=v; imgData.data[i*4+1]=v; imgData.data[i*4+2]=v; imgData.data[i*4+3]=255;
      }
      tctx.putImageData(imgData,0,0);
      // draw scaled to depthCanvas
      depthCtx.clearRect(0,0,depthCanvas.width, depthCanvas.height);
      depthCtx.drawImage(tmp,0,0, depthCanvas.width, depthCanvas.height);

      // build depth array sized W x H to feed 3D builder
      const depthResampled = new Float32Array(W*H);
      // read back scaled data
      const scaled = depthCtx.getImageData(0,0,W,H).data;
      for(let y=0;y<H;y++){
        for(let x=0;x<W;x++){
          const idx = (y*W + x)*4;
          depthResampled[y*W + x] = scaled[idx] / 255;
        }
      }
      buildSurfaceFromDepth(depthResampled, W, H, 60);
      log('AI depth estimation done.');
    }catch(err){
      log('AI pipeline failed — falling back to brightness. Error: ' + (err.message || err));
      const depth = computeDepthFallback(imageData, W, H);
      renderDepthCanvas(depth, W, H);
      buildSurfaceFromDepth(depth, W, H, 40);
    }
  });

  // Reset
  resetBtn.addEventListener('click', ()=>{
    origCtx.clearRect(0,0,origCanvas.width, origCanvas.height);
    depthCtx.clearRect(0,0,depthCanvas.width, depthCanvas.height);
    if(surfaceMesh){ scene.remove(surfaceMesh); surfaceMesh.geometry.dispose(); surfaceMesh.material.dispose(); surfaceMesh=null; }
    log('Reset done.');
  });

  // resize handler for 3D viewport
  window.addEventListener('resize', ()=>{
    if(!renderer) return;
    const W = container3d.clientWidth, H = container3d.clientHeight;
    renderer.setSize(W,H);
    camera.aspect = W/H;
    camera.updateProjectionMatrix();
  });
  </script>
</body>
</html>
